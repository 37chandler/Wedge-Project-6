{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Transaction File Unzipping and GBQ Data Upload "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dependencies \n",
    "\n",
    "import os\n",
    "import re\n",
    "import datetime \n",
    "import zipfile\n",
    "import logging\n",
    "\n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "from google.oauth2 import credentials\n",
    "from google.api_core import exceptions\n",
    "from google.cloud.exceptions import BadRequest\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import janitor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Credentials & Establish Connection\n",
    "\n",
    "gbq_proj_id = \"dow-wedge-transactions\"\n",
    "dataset_id = \"transactions\"  \n",
    "\n",
    "client = bigquery.Client(project=gbq_proj_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List out current contents of GBQ project:\n",
    "\n",
    "datasets = list(client.list_datasets())\n",
    "if datasets:\n",
    "    for item in datasets:\n",
    "        print(item.full_dataset_id)\n",
    "else:\n",
    "    print(\"Project empty, no datasets present\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to see if transaction tables exist in datasets (commented out as files have been uploaded)\n",
    "\n",
    "# Regex pattern to match file naming structure\n",
    "#trans_pattern = re.compile(r\"transArchive_201\\d+\")\n",
    "\n",
    "#tables = client.list_tables(dataset_id)\n",
    "\n",
    "# Flag to track if any deletions were made\n",
    "#deleted_any = False\n",
    "\n",
    "#for table in tables:\n",
    "    # Check if the table ID matches the pattern (starting with 'transArchive_201')\n",
    " #   if trans_pattern.search(table.table_id):\n",
    "        # Construct the full table name using the new project ID\n",
    "  #      full_name = \".\".join([gbq_proj_id, dataset_id, table.table_id])\n",
    "        \n",
    "        # Delete the table from the specified project\n",
    "   #     client.delete_table(full_name, not_found_ok=True)\n",
    "    #    print(f\"Deleted {full_name}.\")\n",
    "\n",
    "        # Set flag to True since a table was deleted\n",
    "     #   deleted_any = True    \n",
    "\n",
    "# If no tables were deleted, print a message indicating no changes were made\n",
    "#if not deleted_any:\n",
    " #   print(\"No tables matched the pattern, no changes made.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script to define our loops to; unzip the transaction files, check to see if the files exist in GBQ, and upload them to GBQ if absent\n",
    "\n",
    "# Set up status/error logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Define paths\n",
    "ZIP_FILE_PATH = r'E:\\files\\wedge-clean-files.zip'\n",
    "DESTINATION_FOLDER = r'E:\\files\\wedge-transactions'\n",
    "\n",
    "# creating sub-loops \n",
    "\n",
    "def unzip_transaction_files(zip_file_path, destination_folder):\n",
    "    # Unzip the archive containing transaction files using Try - Except function \n",
    "    try:\n",
    "        with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(destination_folder)\n",
    "        logging.info(f\"Transaction files unzipped to {destination_folder}\") # using logging across board \n",
    "    except zipfile.BadZipFile:\n",
    "        logging.error(f\"Error: {zip_file_path} is not a valid zip file.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error unzipping transaction files: {str(e)}\")\n",
    "\n",
    "def transaction_file_exists_in_bigquery(file_name, dataset_id):\n",
    "    # Check if a transaction file has already been uploaded to BigQuery using Try - Except function \n",
    "    table_name_pattern = re.compile(re.escape(file_name.split('.')[0]))\n",
    "    try:\n",
    "        tables = list(client.list_tables(dataset_id))\n",
    "        return any(table_name_pattern.search(table.table_id) for table in tables)\n",
    "    except exceptions.NotFound:\n",
    "        logging.error(f\"Dataset {dataset_id} not found.\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error checking transaction file existence in BigQuery: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def upload_transaction_file_to_bigquery(file_path, table_id):\n",
    "    # Upload a transaction file to BigQuery using Try - Except function \n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        autodetect=True,\n",
    "        source_format=bigquery.SourceFormat.CSV,\n",
    "        write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, \"rb\") as source_file:\n",
    "            job = client.load_table_from_file(source_file, table_id, job_config=job_config)\n",
    "        job.result()  # Wait for the job to complete\n",
    "        logging.info(f\"Uploaded transaction file {file_path} to BigQuery table {table_id}\")\n",
    "    except exceptions.BadRequest as e:\n",
    "        logging.error(f\"Error uploading transaction file {file_path}: {str(e)}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Unexpected error uploading transaction file {file_path}: {str(e)}\")\n",
    "\n",
    "def process_transaction_files(zip_file_path, destination_folder, project_id, dataset_id):\n",
    "    \"\"\"Main function to unzip and upload transaction files if necessary.\"\"\"\n",
    "    unzip_transaction_files(zip_file_path, destination_folder)\n",
    "\n",
    "    for root, _, files in os.walk(destination_folder):\n",
    "        for file in files:\n",
    "            if file.endswith(\".csv\"):\n",
    "                table_name = file.split('.')[0].replace('-', '_')  # Replace hyphens with underscores (if exist)\n",
    "                if not transaction_file_exists_in_bigquery(table_name, dataset_id):\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    table_id = f\"{project_id}.{dataset_id}.{table_name}\"\n",
    "                    upload_transaction_file_to_bigquery(file_path, table_id)\n",
    "                else:\n",
    "                    logging.info(f\"Skipping upload for transaction file {file}, corresponding table already exists in BigQuery.\")\n",
    "\n",
    "if __name__ == \"__main__\": # neat name = main trick suggested by Claude (also using Try - Except function)\n",
    "    try:\n",
    "        process_transaction_files(ZIP_FILE_PATH, DESTINATION_FOLDER, gbq_proj_id, dataset_id)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An error occurred during processing: {str(e)}\") # important error logging by file unzip/upload attempt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script to return list of unzipped files not yet uploaded to GBQ, as some files crashed during upload \n",
    "\n",
    "TRANSACTION_FILES_FOLDER = r'E:\\files\\wedge-transactions\\clean-files'\n",
    "\n",
    "def get_bigquery_tables():\n",
    "    # Retrieve a list of all tables in the specified BigQuery dataset (using Try - Except function)\n",
    "    try:\n",
    "        dataset_ref = f\"{gbq_proj_id}.{dataset_id}\"\n",
    "        print(f\"Attempting to list tables in dataset: {dataset_ref}\")\n",
    "        tables = list(client.list_tables(dataset_ref))\n",
    "        table_ids = [table.table_id for table in tables]\n",
    "        print(f\"Retrieved {len(table_ids)} tables from BigQuery: {table_ids}\")\n",
    "        return table_ids\n",
    "    except exceptions.NotFound:\n",
    "        print(f\"Dataset {dataset_id} not found.\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving tables from BigQuery: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "def get_local_transaction_files(folder_path):\n",
    "    # Retrieve a list of all CSV files in the specified folder\n",
    "    files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "    print(f\"Found {len(files)} local CSV files: {files}\")\n",
    "    return files\n",
    "\n",
    "def find_missing_files(local_files, bigquery_tables):\n",
    "    # Identify which local files are not present in BigQuery\n",
    "    missing_files = []\n",
    "    for local_file in local_files:\n",
    "        table_name = local_file.split('.')[0].replace('-', '_')\n",
    "        print(f\"Checking if {local_file} (table name: {table_name}) exists in BigQuery\")\n",
    "        if table_name not in bigquery_tables:\n",
    "            missing_files.append(local_file)\n",
    "            print(f\"{local_file} not found in BigQuery tables\")\n",
    "        else:\n",
    "            print(f\"{local_file} found in BigQuery tables\")\n",
    "    return missing_files\n",
    "\n",
    "def main():\n",
    "    print(\"Starting the script to check for missing files\")\n",
    "    \n",
    "    # Get list of tables in BigQuery\n",
    "    bigquery_tables = get_bigquery_tables()\n",
    "    \n",
    "    # Get list of local transaction files\n",
    "    local_files = get_local_transaction_files(TRANSACTION_FILES_FOLDER)\n",
    "    \n",
    "    # Find missing files\n",
    "    missing_files = find_missing_files(local_files, bigquery_tables)\n",
    "    \n",
    "    # Print results\n",
    "    if missing_files:\n",
    "        print(\"The following transaction files are not yet in BigQuery:\")\n",
    "        for file in missing_files:\n",
    "            print(f\"- {file}\")\n",
    "        print(f\"\\nTotal missing files: {len(missing_files)}\")\n",
    "    else:\n",
    "        print(\"All local transaction files are present in BigQuery.\")\n",
    "    \n",
    "    print(\"Script execution completed\")\n",
    "\n",
    "if __name__ == \"__main__\": # name = main trick again\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script to target uploads at failed files to upload (note: file names under missing_files list is hardcoded based on above script)\n",
    "\n",
    "def upload_to_bigquery(file_path, table_id):\n",
    "    \"\"\"Upload a CSV file to BigQuery.\"\"\"\n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        autodetect=True,\n",
    "        source_format=bigquery.SourceFormat.CSV,\n",
    "        write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, \"rb\") as source_file:\n",
    "            job = client.load_table_from_file(source_file, table_id, job_config=job_config)\n",
    "        \n",
    "        job.result()  # Wait for the job to complete\n",
    "        print(f\"Uploaded {file_path} to {table_id}\")\n",
    "        return True\n",
    "    except BadRequest as e:\n",
    "        print(f\"Error uploading {file_path}: {str(e)}\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error uploading {file_path}: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def main():\n",
    "    # List of missing files (hardcoded file names as opposed to saving file names to list for increased malleability, leaving in oen file name as example)\n",
    "    missing_files = [\n",
    "        'transArchive_201210_201212_clean.csv' # this one file failed to upload a few times, unsure why \n",
    "    ]\n",
    "\n",
    "    successful_uploads = 0\n",
    "    failed_uploads = 0\n",
    "\n",
    "    for file in missing_files:\n",
    "        file_path = os.path.join(TRANSACTION_FILES_FOLDER, file)\n",
    "        table_name = file.split('.')[0].replace('-', '_')\n",
    "        table_id = f\"{gbq_proj_id}.{dataset_id}.{table_name}\"\n",
    "\n",
    "        print(f\"Attempting to upload {file} to {table_id}\")\n",
    "        if upload_to_bigquery(file_path, table_id):\n",
    "            successful_uploads += 1\n",
    "        else:\n",
    "            failed_uploads += 1\n",
    "\n",
    "    print(f\"\\nUpload process completed.\")\n",
    "    print(f\"Successful uploads: {successful_uploads}\")\n",
    "    print(f\"Failed uploads: {failed_uploads}\")\n",
    "\n",
    "if __name__ == \"__main__\": # trick from Claude again\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
